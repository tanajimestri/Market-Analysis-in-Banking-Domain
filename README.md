# Market-Analysis-in-Banking-Domain
Big Data Analysis using Spark

DESCRIPTION

Background and Objective:

Your client, a Portuguese banking institution, ran a marketing campaign to convince potential customers to invest in a bank term deposit scheme. 
The marketing campaigns were based on phone calls. Often, the same customer was contacted more than once through phone, in order to assess if they would want to subscribe to the bank term deposit or not. You have to perform the marketing analysis of the data generated by this campaign.

Analysis tasks completed:

1. Load data and create a Spark data frame
2. Give marketing success rate (No. of people subscribed / total no. of entries)   
3. Give marketing failure rate
4. Give the maximum, mean, and minimum age of the average targeted customer
5. Check the quality of customers by checking average balance, median balance of customers
6. Check if age matters in marketing subscription for deposit
7. Check if marital status mattered for a subscription to deposit
8. Check if age and marital status together mattered for a subscription to deposit scheme
9. Do feature engineering for the bank and find the right age effect on the campaign.

Execution - 

•	loading data and creating spark data frame -

Run spark shell as 
spark-shell --packages com.databricks:spark-csv_2.11:1.5.0
load the file as dataframe using the spark-csv package 

val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val df = sqlContext.read.format("com.databricks.spark.csv").option("header","true").option("inferSchema","true").option("delimiter",",").load("/user/ tanajimestrigmail/project-data.txt")


•	Giving marketing success rate. (No. of people subscribed / total no. of entries) -

  Get the total count of records 
  val totalcount = df.count().toDouble

Get the total subscribed clients 
val subscription_count= df.filter($"y" === "yes").count().toDouble

now find the success rate 
val success_rate = subscription_count/totalcount
SL no column is not required to find the success rate. You can use count inbuilt function to function.

•	Using Count find the number find number of people subscribed, divide by total number people using count.

•	Check max, min, Mean and median age of average targeted customer - 

df.select(max($"age"), avg($"age"), min($"age")).show

•	Check quality of clients by checking average balance, median balance of clients- 

Register the temp table with the name bankdetails -

df.registerTempTable("bankdetails")

now query for the average and median balance from the table -

sqlContext.sql("select percentile(balance,0.5) as median ,avg(balance) as average from bankdetails").show

•	Check if age matters in marketing subscription for deposit- 

Get the average age based on subscriptions as 
df.groupBy("y").agg(avg($"age")).show

•	Check if marital status mattered for subscription to deposit.

Get the count based on the subscription on the marital status 

df.groupBy("y").agg(count($"marital")).show

•	Check if age and marital status together mattered for subscription to deposit scheme -

Do the grouping based on age and the marital status 
df.groupBy("marital","y").count().sort($"count".desc).show

•	Feature engineering for age column and find right age effect on campaign -

df.groupBy("age","y").count().sort($"count".desc).show
+---+---+-----+
|age|  y|count|
+---+---+-----+
| 32| no| 1864|
| 31| no| 1790|
| 33| no| 1762|
| 34| no| 1732|
| 35| no| 1685|
| 36| no| 1611|
| 30| no| 1540|
| 37| no| 1526|
| 39| no| 1344|
| 38| no| 1322|
| 40| no| 1239|
| 41| no| 1171|
| 42| no| 1131|
| 45| no| 1110|
| 43| no| 1058|
| 46| no| 1057|
| 44| no| 1043|
| 29| no| 1014|
| 47| no|  975|
| 48| no|  915|
+---+---+-----+
only showing top 20 rows

df.groupBy("age","y").count().sort($"count".desc).count
res45: Long = 148

age is between 18 and 95 and with possible subscriptions of yes and no, you will get 78*2 records. Better to divide age category as three with 18-30 as young and 31 to 65 as mid and > 65 as old 

create an udf for the conversion

import org.apache.spark.sql.functions.udf

def ageToCategory = udf((age:Int) => {
      age match {
      case t if t < 30 => "young"
      case t if t > 65 => "old"
      case _ => "mid"
      }
      }
     )


Add a new column converting the age to category

val newdf = df.withColumn("agecategory",ageToCategory(df("age")))

newdf: org.apache.spark.sql.DataFrame = [age: int, job: string, marital: string, education: string, default: string, balance: int, housing: string, loan: string, contact: string, day: int, month: string, duration: int, campaign: int, pdays: int, previous: int, poutcome: string, y: string, agecategory: string]

query based on agecategory and subscription, possible records are 3*2, which are easier for analysis

newdf.groupBy("agecategory","y").count().sort($"count".desc).show

+-----------+---+-----+
|agecategory|  y|count|
+-----------+---+-----+
|        mid| no|38889|
|        mid|yes| 4762|
|      young| no|  602|
|        old| no|  431|
|        old|yes|  320|
|      young|yes|  207|
+-----------+---+-----+


# Conclusion: Looks like middle aged clients are much interested.










 
